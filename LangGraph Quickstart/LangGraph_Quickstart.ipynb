{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "Y-gB8ut9YR4Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "czBqZ9HfWPAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d8feefd-9cec-406f-e5de-25ca90d7ba80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.11-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.44)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.20-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.57-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.61.1)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.10.6)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Downloading langgraph-0.3.11-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.5/132.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.8-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.20-py3-none-any.whl (39 kB)\n",
            "Downloading langgraph_prebuilt-0.1.3-py3-none-any.whl (24 kB)\n",
            "Downloading langgraph_sdk-0.1.57-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langgraph-sdk, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph\n",
            "Successfully installed langchain-openai-0.3.8 langgraph-0.3.11 langgraph-checkpoint-2.0.20 langgraph-prebuilt-0.1.3 langgraph-sdk-0.1.57 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langgraph langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langgraph langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd5JTjpTYdVN",
        "outputId": "ab5b1cd4-6f25-4bc9-f77e-1640d78f6b42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langgraph\n",
            "Version: 0.3.11\n",
            "Summary: Building stateful, multi-actor applications with LLMs\n",
            "Home-page: https://www.github.com/langchain-ai/langgraph\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph-sdk\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-openai\n",
            "Version: 0.3.8\n",
            "Summary: An integration package connecting OpenAI and LangChain\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, openai, tiktoken\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Build a Basic Chatbot"
      ],
      "metadata": {
        "id": "swl4VFiXYnRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "# Part1: Build a Basic Chatbot #\n",
        "################################\n",
        "\n",
        "# Start by creating a StateGraph, A State object defines the structure\n",
        "# of our chabot as \"state machine\", We'll add `nodes` to represent the llm\n",
        "# and functions our chatbot can call and `edges` to specifiy how the bot\n",
        "# should transition between these functions\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    # Message have the type \"list\", The \"add_messages\" function\n",
        "    # in the annotation defines how this state should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "    message: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Next, add a \"chatbot\" node, Nodes represent units of work,\n",
        "# They are typically regular python funcitons\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "llm = ChatAnthropic(model=\"\")\n",
        "\n",
        "def chatbot(state: State):\n",
        "    # Notice how the chatbot node function takes the current `State` as\n",
        "    # input adn returns a dictionary containing an updated `messages` list under the key \"messages\".\n",
        "    # This is the basic pattern for all LangGraph node functions\n",
        "    return {\"messages\": [llm.invokes(state[\"messages\"])]}\n",
        "\n",
        "# The first argument is the unique node name\n",
        "# The second argument is the function or object that will be called whenever\n",
        "# The node is used\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "# Next, add an `entry` point.\n",
        "# This tells our graph where to start its work each time we run it.\n",
        "graph_builder.add_edge(START, \"chat_bot\")\n",
        "# Similarly, Set a finish point.\n",
        "# This instructs the graph \"any time this node is run, you can exit.\"\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "# Finally, we'll want to be able to run our graph, call \"compile()\" on the graph builder.\n",
        "# This creates a \"CompiledGraph\" we can use invoke on our state\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "\n",
        "# You can visualize the graph using the `get_graph` method and one of the \"draw\"\n",
        "# methods, like `draw_ascii` or `draw_png`. the `draw` methods each require additional dependencies\n",
        "from IPython.display import Image, display\n",
        "try:\n",
        "    display(Image(graph.get_graph).draw_mermaid_png())\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Now let's run the chatbot!!!\n",
        "def stream_graph_updates(user_input: str):\n",
        "    messages = {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}\n",
        "    for event in graph.stream(messages):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User:\")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        stream_graph_updates(user_input=user_input)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Congratulations!!! You're built your first chatbot using LangGraph.\n",
        "# This bot can engage in basic conversation by taking user input and generating responses using an LLM\n",
        "# However, you may have noticed that the bot's knowledge is limited to what's in its training date. In the\n",
        "# next part, we'll add a web search tool to expand the bot's knowledge and make it more capable.\n",
        "\n",
        "# Blow is the full code for this section for your reference:\n",
        "from typing import Annotated\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    message: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "llm = ChatAnthropic(model=\"\")\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "graph_builder.set_finish_point(\"chatbot\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "MRQgMMAHZPSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Enhancing the Chatbot with Tools"
      ],
      "metadata": {
        "id": "nh4ZQ0MSYrSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################\n",
        "# Part2: Enhancing the Chatbot with Tools #\n",
        "###########################################\n",
        "\n",
        "# To handle queries our chatbot can't answer \"from memory\",\n",
        "# we'll integrate a web search tool. Our bot can use this tool\n",
        "# to find relevant information and provide better responses.\n",
        "\n",
        "# Define the tool:\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "tool = TavilySearchResults(max_result=2)\n",
        "tools = [tool]\n",
        "tool.invoke(\"What's a `node` in LangGraph?\") # The results are pages summaries\n",
        "\n",
        "# Next, we'wll start defining our graph.\n",
        "# The following is all the same as Part1, except we have added `bind_tools` on our LLM.\n",
        "# This lets the LLM konw the correct JSON format to use if it wawnts to use our search engine.\n",
        "from typing import Annotated\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    message: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "llm = ChatAnthropic(model=\"\")\n",
        "# Modification: tell the LLM which tools it can call\n",
        "llm_with_tools = llm.bind(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "#...\n",
        "\n",
        "# Next we need to create a function to actually run the tools if they are called.\n",
        "# We'll do this by adding the tools to a new node.\n",
        "# Below we implement a `BasicToolNode` that checks the most recent messages in the state\n",
        "# and calls tools if the messages contains `tool_calls`. It relies on  the LLM's `tool_calling`\n",
        "# support, which is available in Anthropic, OpenAI...\n",
        "\n",
        "# We will later replace this with LangGraph's prebuilt `ToolNode` to speed\n",
        "# things up, but building it ourselves first is instructive.\n",
        "import json\n",
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "class BasicToolNode:\n",
        "    \"\"\" A node that runs the tools requested in the last AIMessages \"\"\"\n",
        "    def __init__(self, tools:list) -> None:\n",
        "        self.tools_by_name = {tool.name: tool for tool in tool}\n",
        "\n",
        "    def __call__(self, inputs:dict):\n",
        "        if messages := inputs.get(\"messages\", []):\n",
        "            message = messages[-1]\n",
        "        else:\n",
        "            raise ValueError(\"No messages found in input\")\n",
        "        outputs = []\n",
        "        for tool_call in message.tool_calls:\n",
        "            tool_result = self.tools_by_name[tool_call[\"name\"].invoke(tool_call[\"args\"])]\n",
        "        outputs.append(\n",
        "            ToolMessage(\n",
        "                tool_call_id=tool_call[\"id\"],\n",
        "                name=tool[\"name\"],\n",
        "                content=json.dump(tool_result),\n",
        "            )\n",
        "        )\n",
        "        return {\"messages\": outputs}\n",
        "\n",
        "tool_node = BasicToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Below, call define a router function called `route_tools`,\n",
        "# that checks for tool_calls in the chatbot's output.\n",
        "# Provide this function to the graph by calling `add_conditonal_edges`,\n",
        "# which tells the graph that whenever the `chatbot` node completes to check this function to see where to go next.\n",
        "\n",
        "# The condition will route to `tools` if tool calls are present and `END` if not.\n",
        "# Later, we will replace this with the prebuilt `tools_condition` to be more concise,\n",
        "# but implementing it ourselves first makes things more clear.\n",
        "def route_tools(state: State):\n",
        "    \"\"\"\n",
        "    Use in the conditional_edge to route to the ToolNode if the last message\n",
        "    has tool calls. Otherwise, route to the end.\n",
        "    \"\"\"\n",
        "    # 1.如果 state 是一个列表，那么 ai_message 被设置为列表的最后一个元素\n",
        "    if isinstance(state, list):\n",
        "        ai_message = state[-1]\n",
        "    # 2.如果 state 是一个字典，并且包含键 \"messages\"，那么 ai_message 被设置为 state[\"messages\"] 列表的最后一个元素\n",
        "    elif message := state.get(\"messages\", []):\n",
        "        ai_message = message[-1]\n",
        "    else:\n",
        "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
        "    # 3.如果 ai_message 对象有 tool_calls 属性，并且 tool_calls 列表不为空，则返回字符串 \"tools\"\n",
        "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_call) > 0:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool\n",
        "# and \"END\" if it is fine directly responding. This conditional routing defines the main agent loop.\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    route_tools,\n",
        "    # The folloing dictionary lets you tell the graph\n",
        "    # to interpret the condition's outputs as a  specific node\n",
        "    # It defaults to the identity function, but if you want to use a node named something else apart from \"tools\",\n",
        "    # you can update the value of the dictionary to something else\n",
        "    # e.g., \"tools\": \"my_tools\"\n",
        "    {\"tools\": \"tools\", END: END}\n",
        ")\n",
        "\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "# Note that we don't need to explicity set a `finish_point` this time,\n",
        "# because our graph already has a way to finish!\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "# Now we can ask the bot questions outside its training data.\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User:\")\n",
        "        if user_input.lower() in [\"quit\", \"eixt\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        stream_graph_updates(user_input=user_input)\n",
        "    except:\n",
        "        # fallback if input() is not available\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break\n",
        "\n",
        "# Congrats! You've created a conversational agent in langgraph that can use a search engine to retrieve\n",
        "# updated information when needed. Now it can handle a wider range of user queries.\n",
        "\n",
        "# Our chatbot still can't remember past interactions on its own, limiting its ability to have coherent,\n",
        "# multi-turn conversation. In the next part, we'll add memory to address this.\n",
        "\n",
        "# The full code for the graph we've created in this section is reproduced below, replacing our `BasicToolNode`\n",
        "# for the prebuilt `ToolNode`, and our `route_tools` condition with the prebuilt `tool_condition`\n",
        "from typing import Annotated\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "class State(TypedDict):\n",
        "    message: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "tool = TavilySearchResults(max_result=2)\n",
        "tools = [tool]\n",
        "llm = ChatAnthropic(model=\"\")\n",
        "llm_with_tools = llm.bind_tools[tools]\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "\n",
        "# Any time a tool is called,\n",
        "# We return to the chatbot to decide the next step.\n",
        "# !!!Note that we don't need to explicity set a `finish_point` this time,\n",
        "# because our graph already has a way to finish!!!\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "-Patd1_BZnqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Adding Memory to the Chatbot"
      ],
      "metadata": {
        "id": "jcgjc5w3YuPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# Part 3: Adding Memory to the Chatbot #\n",
        "########################################\n",
        "# Our chatbot can now use tools to answer user question, buts it doesn'ts remembers\n",
        "# the context of previouss interactions. This limits its ability to have coherent, multi-turn conversations.\n",
        "\n",
        "# LangGraph solves this problem through persistent checkpointing. If you provide a `checkpointer` when compiling\n",
        "# the graph and `thread_id` when calling your graph, LangGraph automatically saves the sate after each step.\n",
        "# When you invoke the graph again using the same `thread_id`, the graph loads its saved state,\n",
        "# allowing the chatbot to pick up where it left off.\n",
        "\n",
        "# We will see later that checkpointing is much more powerful than simple chat memroy - it lets\n",
        "# you save and resume complex state at any time for error recovery, human-in-loop workflows,\n",
        "# time travel interacions, and more. But before we get too ahead of ourselves, let's add checkpointing\n",
        "# to enable multi-turn conversations.\n",
        "\n",
        "# Notice: we're using an in-memory checkpointer. This is convenient for our tutorial(it saves it all in-memory).\n",
        "# In a production application, you would like change this to use `SqliteSaver` or `PostgresSaver` and connect to\n",
        "# your own DB.\n",
        "\n",
        "# The following is all copied from Part 2.\n",
        "from typing import Annotated\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)"
      ],
      "metadata": {
        "id": "bYFPiWQxaH_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Human-in-the-loop"
      ],
      "metadata": {
        "id": "kX4TQ6fNYymF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5: Customizing State"
      ],
      "metadata": {
        "id": "QdCsPvi8Y3Te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 6: Time Travel"
      ],
      "metadata": {
        "id": "kmhBjtuPY507"
      }
    }
  ]
}
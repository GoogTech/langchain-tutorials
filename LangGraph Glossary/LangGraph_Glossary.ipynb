{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoogTech/langchain-tutorials/blob/master/LangGraph Glossary/LangGraph_Glossary.ipynb\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Refer to the langgraph tutorials of `v0.2.74` : https://langchain-ai.github.io/langgraph/concepts/low_level/"
      ],
      "metadata": {
        "id": "KtmgCMiCVMCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "M1HIx4qkXkJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgRfVlAKXo5p",
        "outputId": "84d0cb05-667c-4416-ead4-b39fade4958d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.74-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.6-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.37)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.16-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.53-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.61.1)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.3.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.10.6)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Downloading langgraph-0.2.74-py3-none-any.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.4/151.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.6-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.16-py3-none-any.whl (38 kB)\n",
            "Downloading langgraph_sdk-0.1.53-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langgraph-sdk, langgraph-checkpoint, langchain-openai, langgraph\n",
            "Successfully installed langchain-openai-0.3.6 langgraph-0.2.74 langgraph-checkpoint-2.0.16 langgraph-sdk-0.1.53 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langgraph langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMPOP5t_X3GG",
        "outputId": "4b2fb12b-0290-439a-d9fc-cc3412fbc738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langgraph\n",
            "Version: 0.2.74\n",
            "Summary: Building stateful, multi-actor applications with LLMs\n",
            "Home-page: https://www.github.com/langchain-ai/langgraph\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, langgraph-checkpoint, langgraph-sdk\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-openai\n",
            "Version: 0.3.6\n",
            "Summary: An integration package connecting OpenAI and LangChain\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, openai, tiktoken\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Graphs"
      ],
      "metadata": {
        "id": "N3MZ17G8V0oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In short:\n",
        "* nodes do the work.\n",
        "* edges tell what to do next.\n",
        "* state represents the memory that records what you did."
      ],
      "metadata": {
        "id": "J4DtCJDwb1ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1.StateGraph"
      ],
      "metadata": {
        "id": "T0EHwIFxao3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.Compiling your graph"
      ],
      "metadata": {
        "id": "Ied3wE4qc57j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = graph_builder.compile(...)"
      ],
      "metadata": {
        "id": "O2145DM7eyXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.State"
      ],
      "metadata": {
        "id": "XWI3n13mV4Td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.Schema"
      ],
      "metadata": {
        "id": "rPpaU9Jhf8UQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1.How to use Pydantic model as graph state"
      ],
      "metadata": {
        "id": "MyufPjtsgYsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://langchain-ai.github.io/langgraph/how-tos/state-model/"
      ],
      "metadata": {
        "id": "c3Xwuo9Egp_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `StateGraph` accepts a state_schema argument on initialization `that` specifies the \"shape\" of the state `that` the nodes in the graph can access and update."
      ],
      "metadata": {
        "id": "Mq5mbP4ZmCXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input Validation：**"
      ],
      "metadata": {
        "id": "EAA-aeLthr0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# The overall state of the graph(this is the public state shared across nodes)\n",
        "class OverallState(BaseModel):\n",
        "  a: str\n",
        "\n",
        "def node(state: OverallState):\n",
        "  return {\"a\": \"goodbye\"}\n",
        "\n",
        "# Build the state graph:\n",
        "builder = StateGraph(OverallState)\n",
        "builder.add_node('node', node)\n",
        "builder.add_edge(START, 'node')\n",
        "builder.add_edge('node', END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# Test the graph with a valid input\n",
        "graph.invoke({\"a\": \"hello\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LJJ2Q77h2qr",
        "outputId": "4916513d-50ec-4d63-e36f-316655e37d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 'goodbye'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invoke the graph with an invalid input"
      ],
      "metadata": {
        "id": "LvZmUbgGj2eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  graph.invoke({\"a\": 123})\n",
        "except Exception as e:\n",
        "    print(\"An exception was raised because `a` is an integer rather than a string.\\n\")\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KMTqa2pj5Ff",
        "outputId": "e49dab76-2d92-4543-b63c-865f1a26c84d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An exception was raised because `a` is an integer rather than a string.\n",
            "\n",
            "1 validation error for OverallState\n",
            "a\n",
            "  Input should be a valid string [type=string_type, input_value=123, input_type=int]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple Nodes：**"
      ],
      "metadata": {
        "id": "dS99tINkkXNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# The overall state of the graph (this is the public state shared across nodes)\n",
        "class OverallState(BaseModel):\n",
        "    a: str\n",
        "\n",
        "def bad_node(state: OverallState):\n",
        "    return { # !!!returns an update to the state!!!\n",
        "        \"a\": 123  # Invalid\n",
        "    }\n",
        "\n",
        "def ok_node(state: OverallState):\n",
        "    return {\"a\": \"goodbye\"}\n",
        "\n",
        "# Build the state graph\n",
        "builder = StateGraph(OverallState)\n",
        "builder.add_node(bad_node)\n",
        "builder.add_node(ok_node)\n",
        "builder.add_edge(START, \"bad_node\")\n",
        "builder.add_edge(\"bad_node\", \"ok_node\")\n",
        "builder.add_edge(\"ok_node\", END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# Test the graph with a valid input\n",
        "try:\n",
        "    graph.invoke({\"a\": \"hello\"}) # !!!the validation error will occur when ok_node is called!!!\n",
        "except Exception as e:\n",
        "    print(\"An exception was raised because bad_node sets `a` to an integer.\")\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udrCOdVplMyG",
        "outputId": "bd40a6f5-8189-40da-9453-7cbbe32ca450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An exception was raised because bad_node sets `a` to an integer.\n",
            "1 validation error for OverallState\n",
            "a\n",
            "  Input should be a valid string [type=string_type, input_value=123, input_type=int]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2.How to define input/output schema for your graph"
      ],
      "metadata": {
        "id": "84zhFXB8gl0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://langchain-ai.github.io/langgraph/how-tos/input_output_schema/"
      ],
      "metadata": {
        "id": "nL9YZZMNgr5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema.\n",
        "\n",
        "However, it's also possible to define distinct input and output schemas for a graph."
      ],
      "metadata": {
        "id": "mDathDvVn9An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Define the schema for the input.\n",
        "# The input schema ensures that the provided input\n",
        "# matches the expected structure.\n",
        "class InputState(TypedDict):\n",
        "    question: str\n",
        "\n",
        "# Define the schema for the output.\n",
        "# The output schema filters the internal data to return\n",
        "# only the relevant information according to the defined output schema.\n",
        "class OutputState(TypedDict):\n",
        "    answer: str\n",
        "\n",
        "# Define the overall schema, combining both input and output\n",
        "class OverallState(InputState, OutputState):\n",
        "    pass\n",
        "\n",
        "# Define the node that processes the input and generates an answer\n",
        "def node(state: OverallState):\n",
        "    return {\"question\": state[\"question\"], \"answer\": \"This is answer...\"}\n",
        "\n",
        "# Build the graph with input and output schemas specified\n",
        "builder = StateGraph(OverallState, input=InputState, output=OutputState)\n",
        "builder.add_node(node)\n",
        "builder.add_edge(START, \"node\")\n",
        "builder.add_edge(\"node\", END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# Invoke the graph with and input and print the result.\n",
        "# Notice that the output of invoke only includes the output schema.\n",
        "print(graph.invoke({\"question\": \"What is the meaning of life?\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxzhCg9WpEI-",
        "outputId": "fff179a6-0c11-4126-899b-fdc168c6d185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'This is answer...'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple schemas"
      ],
      "metadata": {
        "id": "0a80W0Gfwg4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we define an \"internal\" schema that contains all keys relevant to graph operations.\n",
        "\n",
        "But, we also define input and output schemas that are sub-sets of the \"internal\" schema to constrain the input and output of the graph."
      ],
      "metadata": {
        "id": "x3SbELyxc28U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class InputState(TypedDict):\n",
        "  user_input: str # shared with the schema of `OverallState`\n",
        "\n",
        "class OutputState(TypedDict):\n",
        "  graph_output: str # shared with the schema of `OverallState`\n",
        "\n",
        "class OverallState(InputState, OutputState):\n",
        "  foo: str\n",
        "  user_input: str\n",
        "  graph_output: str\n",
        "\n",
        "class PrivateState(TypedDict):\n",
        "  bar: str # private variable, because any one of schema doesn't include it.\n",
        "\n",
        "#################################################################################################\n",
        "# Sate Process: User ————> InputState ——> OverallState ——> PrivateSate ——> OutputState ——> User #\n",
        "#################################################################################################\n",
        "def node_1(state: InputState) -> OverallState:\n",
        "  # Read from InputState, write to OverallState\n",
        "  return {\"foo\": state[\"user_input\"] + \" name\"}\n",
        "\n",
        "def node_2(state: OverallState) -> PrivateState:\n",
        "  # Read from OverallState, write to PrivateState\n",
        "  return {\"bar\": state[\"foo\"] + \" is\"}\n",
        "\n",
        "def node_3(state: PrivateState) -> OutputState:\n",
        "  # Read from PrivateState, write to OutputState\n",
        "  return {\"graph_output\": state[\"bar\"] + \" Lance\"}\n",
        "\n",
        "builder = StateGraph(OverallState, input=InputState, output=OutputState)\n",
        "builder.add_node(node_1)\n",
        "builder.add_node(node_2)\n",
        "builder.add_node(node_3)\n",
        "builder.add_edge(START, \"node_1\")\n",
        "builder.add_edge(\"node_1\", \"node_2\")\n",
        "builder.add_edge(\"node_2\", \"node_3\")\n",
        "builder.add_edge(\"node_3\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "graph.invoke({\"user_input\": \"My\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV7HHqNBc6IT",
        "outputId": "c582fefa-4bdf-4064-8471-514e387fcce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'graph_output': 'My name is Lance'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to pass private state between nodes"
      ],
      "metadata": {
        "id": "hxBcSX8yxBOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# The overall state of the graph(this is the public state shared across nodes)\n",
        "class OverallState(TypedDict):\n",
        "  a: str\n",
        "\n",
        "# Output from node_1 contains private data that is not part of the overall state\n",
        "class Node1Output(TypedDict):\n",
        "  private_data: str\n",
        "\n",
        "# Node 2 input onlu requests the private data available after node_1\n",
        "class Node2Input(TypedDict):\n",
        "  private_data: str\n",
        "\n",
        "#################################################################################################################\n",
        "# Sate Process: User ——> OverallState ——> Node1Output ~~~ Node2Input ——> OverallState ——> OverallState ——> User #\n",
        "#################################################################################################################\n",
        "# The private data is only shared between node_1 and node_2\n",
        "def node_1(state: OverallState) -> Node1Output:\n",
        "  output = {\"private_data\": \"set by node_1\"}\n",
        "  print(f\"Entered node `node_1`: \\n\\tInput: {state}.\\n\\tReturn:{output}\")\n",
        "  return output\n",
        "\n",
        "def node_2(state: Node2Input) -> OverallState:\n",
        "  output = {\"a\": \"set by node_2\"}\n",
        "  print(f\"Entered node `node_2`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n",
        "  return output\n",
        "\n",
        "# Node 3 only has access to the overall state (!!!no access to private data from node_1!!!)\n",
        "def node_3(state: OverallState) -> OverallState:\n",
        "  output = {\"a\": \"set by node_3\"}\n",
        "  print(f\"Entered node `node_3`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n",
        "  return output\n",
        "\n",
        "# Build the state graph\n",
        "builder = StateGraph(OverallState)\n",
        "builder.add_node(node_1) # node_1 is the first node\n",
        "builder.add_node(node_2) # node_2 is the second node and accepts private data from node_1\n",
        "builder.add_node(node_3) # node_3 is the third node and does not see the private data\n",
        "builder.add_edge(START, \"node_1\")\n",
        "builder.add_edge(\"node_1\", \"node_2\")\n",
        "builder.add_edge(\"node_2\", \"node_3\")\n",
        "builder.add_edge(\"node_3\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "# Invoke the graph with the initial state\n",
        "response = graph.invoke({\"a\": \"hello\"})\n",
        "print(f\"\\nOutput of graph invocation: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goeukIs405zW",
        "outputId": "d87e1e04-03fd-476e-a5da-11fa1892df5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entered node `node_1`: \n",
            "\tInput: {'a': 'hello'}.\n",
            "\tReturn:{'private_data': 'set by node_1'}\n",
            "Entered node `node_2`:\n",
            "\tInput: {'private_data': 'set by node_1'}.\n",
            "\tReturned: {'a': 'set by node_2'}\n",
            "Entered node `node_3`:\n",
            "\tInput: {'a': 'set by node_2'}.\n",
            "\tReturned: {'a': 'set by node_3'}\n",
            "\n",
            "Output of graph invocation: {'a': 'set by node_3'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.Reducers"
      ],
      "metadata": {
        "id": "ZBoopCPtnJnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example A:"
      ],
      "metadata": {
        "id": "5xVDP-E9pB6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "class State(TypedDict):\n",
        "    foo: int\n",
        "    bar: list[str]"
      ],
      "metadata": {
        "id": "C7nlq53HpEj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example B:"
      ],
      "metadata": {
        "id": "m84SfhFOp6uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import  TypedDict\n",
        "from operator import  add\n",
        "\n",
        "class State(TypedDict):\n",
        "    foo: int\n",
        "    bar: Annotated[list[str], add]"
      ],
      "metadata": {
        "id": "FAKaiLz5paAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.Working with Messages in Graph State"
      ],
      "metadata": {
        "id": "L-knyYDEqVF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.1.Messages"
      ],
      "metadata": {
        "id": "I0UdkO3d6LxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider."
      ],
      "metadata": {
        "id": "_YJVPk71yqWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The five main message types are :\n",
        "\n",
        "* SystemMessage: corresponds to `system` role\n",
        "* HumanMessage: corresponds to `user` role\n",
        "* AIMessage: corresponds to `assistant` role\n",
        "* AIMessageChunk: corresponds to `assistant` role, used for `streaming` responses\n",
        "* ToolMessage: corresponds to `tool` role"
      ],
      "metadata": {
        "id": "ZAUNVyHoyxSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HumanMessage :"
      ],
      "metadata": {
        "id": "g5Ro6C1Mzo7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HuamnMessage\n",
        "model.invoke([HuamanMessage(content=\"hello, how are you?\")])\n",
        "\n",
        "# Or:\n",
        "# When invoking a chat model with a string as input,\n",
        "# LangChain will automatically convert the string into a HumanMessage object.\n",
        "# This is mostly useful for quick testing.\n",
        "model.invoke(\"hello, how are you?\")"
      ],
      "metadata": {
        "id": "-EwR5ieNzUvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AIMessage :"
      ],
      "metadata": {
        "id": "qlVXxHv0zzti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "ai_message = model.invoke([HumanMessage(content=\"Tell me a joke\")])\n",
        "ai_message # <-- AIMessage"
      ],
      "metadata": {
        "id": "90tNwCeA0fAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AIMessageChunk :"
      ],
      "metadata": {
        "id": "MKxkYFcv1MUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in model.stream([HuamnMessage(\"What color is the sky?\")]):\n",
        "  print(chunk)"
      ],
      "metadata": {
        "id": "05jjSmzl1NdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI Format :\n",
        "Chat models also accept OpenAI's format as inputs to chat models"
      ],
      "metadata": {
        "id": "3WHj8-XM1yZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke([\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Hello, how are you?\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"I'm doing well, thank you for asking.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Can you tell me a joke?\",\n",
        "    }\n",
        "])"
      ],
      "metadata": {
        "id": "BgEi9ODV11C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.2.Serialization"
      ],
      "metadata": {
        "id": "w91I9DrVtIH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AnyMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class State(TypedDict):\n",
        "  # the state updates are always deserialized into LangChain Messages\n",
        "  # when using add_messages\n",
        "  messages: Annotated[list[AnyMessage], add_messages]"
      ],
      "metadata": {
        "id": "0YZ_gfc75KbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.3.MessagesState"
      ],
      "metadata": {
        "id": "-AcJ-hcP6YTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "\n",
        "# MessagesState is defined with a single messages key\n",
        "# which is a list of AnyMessage objects and uses the add_messages reducer.\n",
        "class State(MessagesState):\n",
        "  # Typically, there is more state to track than just messages,\n",
        "  # so we see people subclass this state and add more fields, like:\n",
        "  documents: list[str]"
      ],
      "metadata": {
        "id": "UwPFx1_v6ttM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Nodes"
      ],
      "metadata": {
        "id": "JIIoUWVbV-sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "builder = StateGraph(dict)https://langchain-ai.github.io/langgraph/concepts/low_level/#start-node\n",
        "\n",
        "# the first positional argument is the state,\n",
        "# and (optionally), the second positional argument is a \"config\",\n",
        "def my_node(state: dict, config: RunnableConfig):\n",
        "  print(\"In node:\", config[\"configurable\"][\"user_id\"])\n",
        "  return {\"results\": f\"Hello, {state['input']}\"}\n",
        "\n",
        "# The second argument is optional\n",
        "def my_other_node(state: dict):\n",
        "  return state\n",
        "\n",
        "builder.add_node('my_node', my_node)\n",
        "# If you add a node to a graph without specifying a name,\n",
        "# it will be given a default name equivalent to the function name.\n",
        "builder.add_node(my_other_node)"
      ],
      "metadata": {
        "id": "d9yjdvy_9Kf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.START Node"
      ],
      "metadata": {
        "id": "ee80h594-uqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START\n",
        "\n",
        "#  The main purpose for referencing this node is\n",
        "# to determine which nodes should be called first.\n",
        "builder.add_edge(START, \"my_node\")"
      ],
      "metadata": {
        "id": "8goIq7AY-3xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.END Node"
      ],
      "metadata": {
        "id": "2NiS_zxQ_LED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END\n",
        "\n",
        "# This node is referenced when you want to denote\n",
        "# which edges have no actions after they are done.\n",
        "graph.add_edge(\"node_a\", END)"
      ],
      "metadata": {
        "id": "1a7HnIxG_OXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Edges"
      ],
      "metadata": {
        "id": "aPJ8WEpeWa1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.Normal Edges\n"
      ],
      "metadata": {
        "id": "oeuJCVZFAr2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.add_edge(\"node_a\", \"node_b\")"
      ],
      "metadata": {
        "id": "m3FE7yGiAv-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.Conditional Edges"
      ],
      "metadata": {
        "id": "z6_ty0naA1Qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# By default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next.\n",
        "# All those nodes will be run in parallel as a part of the next superstep.\n",
        "graph.add_conditional_edges(\"node_a\", routing_function)\n",
        "\n",
        "# Or you can optionally provide a dictionary that maps the routing_function's output to the name of the next node.\n",
        "graph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_a\"})"
      ],
      "metadata": {
        "id": "6goaMoUlA6YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Send"
      ],
      "metadata": {
        "id": "taltLzySWeBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def continue_to_jokes(state: OverallState):\n",
        "  # Send takes two arguments:\n",
        "  # first is the name of the node, and second is the state to pass to that node.\n",
        "  return [Send(\"generate_joke\", {\"subject\": s}) for s in state['subjects']]\n",
        "\n",
        "graph.add_conditional_edges('node_a', continue_to_jokes)"
      ],
      "metadata": {
        "id": "xxAESxXdFTQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Command"
      ],
      "metadata": {
        "id": "D8xkHEP1WgJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  you might want to BOTH perform state updates AND decide\n",
        "# which node to go to next in the SAME node.\n",
        "def my_node(state: State) -> Command[Literals[\"my_other_node\"]]:\n",
        "  return Command(\n",
        "      # state udpate\n",
        "      update = (\"foo\": \"bar\"),\n",
        "      # control flow\n",
        "      goto=\"my_other_node\"\n",
        "  )\n",
        "\n",
        "# With `Command` you can also achieve dynamic control flow behavior (identical to conditional edges)\n",
        "def my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n",
        "  if state[\"foo\"] == \"bar\":\n",
        "    return Command(\n",
        "        update = {\"foo\": \"baz\"},\n",
        "        goto = \"my_other_node\"\n",
        "    )"
      ],
      "metadata": {
        "id": "O1OR0_XGHcer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.When should I use Command instead of conditional edges?"
      ],
      "metadata": {
        "id": "_Oq_0w3-KC94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Command when you need to both update the graph state and route to a different node.\n",
        "\n",
        "For example, when implementing multi-agent handoffs where it's important to route to a different agent and pass some information to that agent."
      ],
      "metadata": {
        "id": "_QrxKNt_KYsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2.Navigating to a node in a parent graph"
      ],
      "metadata": {
        "id": "W_JwIMmNL03a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_node(state: State) -> Command[Literal['my_other_node']]:\n",
        "  return Command(\n",
        "      update = {\"foo\": \"bar\"},\n",
        "      goto = \"my_other_node\", # # where `other_subgraph` is a node in the parent graph\n",
        "      graph = Command.PARENT\n",
        "  )"
      ],
      "metadata": {
        "id": "ITxc_1rRNvzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.Using inside tools"
      ],
      "metadata": {
        "id": "3l8CtX1yQKKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\n",
        "    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\n",
        "    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\n",
        "    return Command(\n",
        "        update={\n",
        "            # update the state keys\n",
        "            \"user_info\": user_info,\n",
        "            # update the message history\n",
        "            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "KId4WF6iQSSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4.Human-in-the-loop"
      ],
      "metadata": {
        "id": "rDfw2O18l6YL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4.1.Use cases"
      ],
      "metadata": {
        "id": "u5MCLVIymL_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 🛠️ Reviewing tool calls: Humans can review, edit, or approve tool calls requested by the LLM before tool execution.\n",
        "\n",
        "* ✅ Validating LLM outputs: Humans can review, edit, or approve content generated by the LLM.\n",
        "\n",
        "* 💡 Providing context: Enable the LLM to explicitly request human input for clarification or additional details or to support multi-turn conversations."
      ],
      "metadata": {
        "id": "btPlyYcFn9Hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4.2.interrupt"
      ],
      "metadata": {
        "id": "ZM9KeyaIoC97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict\n",
        "import uuid\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.constants import START\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.types import interrupt, Command\n",
        "\n",
        "class State(TypedDict):\n",
        "   \"\"\"The graph state.\"\"\"\n",
        "   some_text: str\n",
        "\n",
        "def human_node(state: State):\n",
        "   value = interrupt(\n",
        "      # Any JSON serializable value to surface to the human.\n",
        "      # For example, a question or a piece of text or a set of keys in the state\n",
        "      {\n",
        "         \"text_to_revise\": state[\"some_text\"]\n",
        "      }\n",
        "   )\n",
        "   return {\n",
        "      # Update the state with the human's input\n",
        "      \"some_text\": value\n",
        "   }\n",
        "\n",
        "# Build the graph\n",
        "graph_builder = StateGraph(State)\n",
        "# Add the human-node to the graph\n",
        "graph_builder.add_node(\"human_node\", human_node)\n",
        "graph_builder.add_edge(START, \"human_node\")\n",
        "\n",
        "# A checkpointer is required for `interrupt` to work.\n",
        "checkpointer = MemorySaver()\n",
        "graph = graph_builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "# Pass a thread ID to the graph to run it.\n",
        "thread_config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
        "\n",
        "# Using stream() to directly surface the `__interrupt__` information.\n",
        "for chunk in graph.stream({\"some_text\": \"Original text\"}, config=thread_config):\n",
        "   print(chunk)\n",
        "\n",
        "# Resume using Command\n",
        "for chunk in graph.stream(Command(resume=\"Edited text\"), config=thread_config):\n",
        "   print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb-1CQzIp5Xw",
        "outputId": "328b4083-090b-4dd9-b6c8-e8161ddef283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'__interrupt__': (Interrupt(value={'text_to_revise': 'Original text'}, resumable=True, ns=['human_node:e969b41f-65d8-65b5-43c0-9fec2ec4c437'], when='during'),)}\n",
            "{'human_node': {'some_text': 'Edited text'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4.3.Requirements"
      ],
      "metadata": {
        "id": "__IuyYNHzgqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use interrupt in your graph, you need to:\n",
        "\n",
        "1. Specify a `checkpointer` to save the graph state after each step.\n",
        "2. Call `interrupt()` in the appropriate place. See the Design Patterns section for examples.\n",
        "3. `Run the graph` with a `thread ID` until the interrupt is hit.\n",
        "4. `Resume execution` using invoke/ainvoke/stream/astream"
      ],
      "metadata": {
        "id": "iDWMAOjUzxiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4.4.Desgin Patterns"
      ],
      "metadata": {
        "id": "Gy9-d__O0WeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.4.4.1.Approve or Reject"
      ],
      "metadata": {
        "id": "C6i41ue21pxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "from langgraph.types import interrupt, Command\n",
        "\n",
        "def human_approval(state: State) -> Command[Literal[\"some_node\", \"another_node\"]]:\n",
        "    is_approved = interrupt(\n",
        "        {\n",
        "            \"question\": \"Is this correct?\",\n",
        "            # Surface the output that should be\n",
        "            # reviewed and approved by the human.\n",
        "            \"llm_output\": state[\"llm_output\"]\n",
        "        }\n",
        "    )\n",
        "    if is_approved:\n",
        "        return Command(goto=\"some_node\")\n",
        "    else:\n",
        "        return Command(goto=\"another_node\")\n",
        "\n",
        "# Add the node to the graph in an appropriate location\n",
        "# and connect it to the relevant nodes.\n",
        "graph_builder.add_node(\"human_approval\", human_approval)\n",
        "graph = graph_builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "# After running the graph and hitting the interrupt, the graph will pause.\n",
        "# Resume it with either an approval or rejection.\n",
        "thread_config = {\"configurable\": {\"thread_id\": \"some_id\"}}\n",
        "graph.invoke(Command(resume=True), config=thread_config)"
      ],
      "metadata": {
        "id": "0dZr3F261ozq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.4.4.2.Review & Edit State\n"
      ],
      "metadata": {
        "id": "UVn28Uap5tRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.types import interrupt\n",
        "\n",
        "def human_editing(state: State):\n",
        "    ...\n",
        "    result = interrupt(\n",
        "        # Interrupt information to surface to the client.\n",
        "        # Can be any JSON serializable value.\n",
        "        {\n",
        "            \"task\": \"Review the output from the LLM and make any necessary edits.\",\n",
        "            \"llm_generated_summary\": state[\"llm_generated_summary\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Update the state with the edited text\n",
        "    return {\n",
        "        \"llm_generated_summary\": result[\"edited_text\"]\n",
        "    }\n",
        "\n",
        "# Add the node to the graph in an appropriate location\n",
        "# and connect it to the relevant nodes.\n",
        "graph_builder.add_node(\"human_editing\", human_editing)\n",
        "graph = graph_builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "...\n",
        "\n",
        "# After running the graph and hitting the interrupt, the graph will pause.\n",
        "# Resume it with the edited text.\n",
        "thread_config = {\"configurable\": {\"thread_id\": \"some_id\"}}\n",
        "graph.invoke(\n",
        "    Command(resume={\"edited_text\": \"The edited text\"}),\n",
        "    config=thread_config\n",
        ")"
      ],
      "metadata": {
        "id": "mfjkrNqm6AhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.4.4.3.Review Tool Calls"
      ],
      "metadata": {
        "id": "3h2RDh297yFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def human_review_node(state) -> Command[Literal[\"call_llm\", \"run_tool\"]]:\n",
        "    # This is the value we'll be providing via Command(resume=<human_review>)\n",
        "    human_review = interrupt(\n",
        "        {\n",
        "            \"question\": \"Is this correct?\",\n",
        "            # Surface tool calls for review\n",
        "            \"tool_call\": tool_call\n",
        "        }\n",
        "    )\n",
        "    review_action, review_data = human_review\n",
        "\n",
        "    # Approve the tool call and continue\n",
        "    if review_action == \"continue\":\n",
        "        return Command(goto=\"run_tool\")\n",
        "\n",
        "    # Modify the tool call manually and then continue\n",
        "    elif review_action == \"update\":\n",
        "        ...\n",
        "        updated_msg = get_updated_msg(review_data)\n",
        "        # Remember that to modify an existing message you will need\n",
        "        # to pass the message with a matching ID.\n",
        "        return Command(goto=\"run_tool\", update={\"messages\": [updated_message]})\n",
        "\n",
        "    # Give natural language feedback, and then pass that back to the agent\n",
        "    elif review_action == \"feedback\":\n",
        "        ...\n",
        "        feedback_msg = get_feedback_msg(review_data)\n",
        "        return Command(goto=\"call_llm\", update={\"messages\": [feedback_msg]})"
      ],
      "metadata": {
        "id": "19v80sd171yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.4.4.4.Multi-turn conversation"
      ],
      "metadata": {
        "id": "k3jQhX6b71Eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a human node per agent :"
      ],
      "metadata": {
        "id": "c6ptFJC1ABQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.types import interrupt\n",
        "\n",
        "def human_input(state: State):\n",
        "    human_message = interrupt(\"human_input\")\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"human\",\n",
        "                \"content\": human_message\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "def agent(state: State):\n",
        "    # Agent logic\n",
        "    ...\n",
        "\n",
        "graph_builder.add_node(\"human_input\", human_input)\n",
        "graph_builder.add_edge(\"human_input\", \"agent\")\n",
        "graph = graph_builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "# After running the graph and hitting the interrupt, the graph will pause.\n",
        "# Resume it with the human's input.\n",
        "graph.invoke(\n",
        "    Command(resume=\"hello!\"),\n",
        "    config=thread_config\n",
        ")"
      ],
      "metadata": {
        "id": "-G_LR8ftAHD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sharing human node across multiple agents :"
      ],
      "metadata": {
        "id": "jFSRPeq6CDs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.types import interrupt\n",
        "\n",
        "def human_node(state: MessagesState) -> Command[Literal[\"agent_1\", \"agent_2\", ...]]:\n",
        "    \"\"\"A node for collecting user input.\"\"\"\n",
        "    user_input = interrupt(value=\"Ready for user input.\")\n",
        "\n",
        "    # Determine the **active agent** from the state, so\n",
        "    # we can route to the correct agent after collecting input.\n",
        "    # For example, add a field to the state or use the last active agent.\n",
        "    # or fill in `name` attribute of AI messages generated by the agents.\n",
        "    active_agent = ...\n",
        "\n",
        "    return Command(\n",
        "        update={\n",
        "            \"messages\": [{\n",
        "                \"role\": \"human\",\n",
        "                \"content\": user_input,\n",
        "            }]\n",
        "        },\n",
        "        goto=active_agent,\n",
        "    )"
      ],
      "metadata": {
        "id": "gDRp7ic5CLOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.4.4.5.Multi-turn conversation"
      ],
      "metadata": {
        "id": "cc0EmcVFDlhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.types import interrupt\n",
        "\n",
        "def human_node(state: State):\n",
        "    \"\"\"Human node with validation.\"\"\"\n",
        "    question = \"What is your age?\"\n",
        "\n",
        "    while True:\n",
        "        answer = interrupt(question)\n",
        "\n",
        "        # Validate answer, if the answer isn't valid ask for input again.\n",
        "        if not isinstance(answer, int) or answer < 0:\n",
        "            question = f\"'{answer} is not a valid age. What is your age?\"\n",
        "            answer = None\n",
        "            continue\n",
        "        else:\n",
        "            # If the answer is valid, we can proceed.\n",
        "            break\n",
        "\n",
        "    print(f\"The human in the loop is {answer} years old.\")\n",
        "    return {\n",
        "        \"age\": answer\n",
        "    }"
      ],
      "metadata": {
        "id": "8ncYs017Dm3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4.5.The Command primitive"
      ],
      "metadata": {
        "id": "b_OI8NNHG7-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Pass a value to the `interrput`:"
      ],
      "metadata": {
        "id": "dIhwTP9qILoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume graph execution with the user's input.\n",
        "graph.invoke(\n",
        "    Command(resume={\"age\": \"25\"}),\n",
        "    thread_config\n",
        ")"
      ],
      "metadata": {
        "id": "pqZFOzrDIREt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Update the graph state:"
      ],
      "metadata": {
        "id": "h0pmixcpIO0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the graph state and resume.\n",
        "# You must provide a `resume` value if using an `interrupt`.\n",
        "graph.invoke(\n",
        "    Command(\n",
        "        update={\"age\": \"25\"},\n",
        "        resume=\"Let's go!!!\",\n",
        "    ),\n",
        "    thread_config\n",
        ")"
      ],
      "metadata": {
        "id": "qNcq0uIfIeDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4.6.Using with `invoke` and `ainvoke`"
      ],
      "metadata": {
        "id": "5kZc5F-qJI2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the graph up to the interrupt\n",
        "result = graph.invoke(inputs, thread_config)\n",
        "\n",
        "# Get the graph state to get interrupt information.\n",
        "state = graph.get_state(thread_config)\n",
        "# Print the state values\n",
        "print(state.values)\n",
        "# Print the pending tasks\n",
        "print(state.tasks)\n",
        "\n",
        "# Resume the graph with the user's input.\n",
        "graph.invoke(Command(resume={\"age\": \"25\"}), thread_config)"
      ],
      "metadata": {
        "id": "46ahO3MPKKwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# {'foo': 'bar'} # State values\n",
        "# (\n",
        "#     PregelTask(\n",
        "#         id='5d8ffc92-8011-0c9b-8b59-9d3545b7e553',\n",
        "#         name='node_foo',\n",
        "#         path=('__pregel_pull', 'node_foo'),\n",
        "#         error=None,\n",
        "#         interrupts=(Interrupt(value='value_in_interrupt', resumable=True, ns=['node_foo:5d8ffc92-8011-0c9b-8b59-9d3545b7e553'], when='during'),), state=None,\n",
        "#         result=None\n",
        "#     ),\n",
        "# ) # Pending tasks. interrupts"
      ],
      "metadata": {
        "id": "CjJQdrzmKR3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4.7.How does resuming from an interrupt work?"
      ],
      "metadata": {
        "id": "_CRTylK2MhGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "def node(state: State):\n",
        "    # All the code from the beginning of the node to the interrupt will be re-executed\n",
        "    # when the graph resumes.\n",
        "    global counter\n",
        "    counter += 1\n",
        "    print(f\"> Entered the node: {counter} # of times\")\n",
        "    # Pause the graph and wait for user input.\n",
        "    answer = interrupt()\n",
        "    print(\"The value of counter is:\", counter)\n",
        "    ..."
      ],
      "metadata": {
        "id": "BNfzfETVMnpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# > Entered the node: 2 # of times\n",
        "# The value of counter is: 2"
      ],
      "metadata": {
        "id": "e6raMEBjMst3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4.8.Common Pitfalls"
      ],
      "metadata": {
        "id": "RBjtBEBmNdeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.4.8.1.Side-effects"
      ],
      "metadata": {
        "id": "C3whfWbFNkQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side effects before interrupt(`BAD`) :"
      ],
      "metadata": {
        "id": "kCl9KTut3pZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.types import interrupt\n",
        "\n",
        "def huamn_node(state: State):\n",
        "  \"\"\"Human node with validation\"\"\"\n",
        "  api_call(...) # !!! This code will be re-executed when the node is resumed !!!\n",
        "  answer = interrupt(question)"
      ],
      "metadata": {
        "id": "xKfyY29sOyky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side effects after interrupt(`OK`) :"
      ],
      "metadata": {
        "id": "jZERrkvw4I-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.types import interrupt\n",
        "\n",
        "def huamn_node(state: State):\n",
        "  \"\"\"Human node with validation\"\"\"\n",
        "  answer = interrupt(question)\n",
        "  api_call(...) # # OK as it's after the interrupt"
      ],
      "metadata": {
        "id": "UMDLQMi74Nwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side effects in a separade node(`OK`) :\n",
        "\n"
      ],
      "metadata": {
        "id": "-yePl_a84blD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.types import interrupt\n",
        "\n",
        "def huamn_node(state: State):\n",
        "  \"\"\"Human node with validation\"\"\"\n",
        "  answer = interrupt(question)\n",
        "\n",
        "def api_call_node(state: State):\n",
        "  api_call(...) # OK as it's after the interrupt"
      ],
      "metadata": {
        "id": "qT45bxr54iG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.4.8.2.Subgraphs called as functions"
      ],
      "metadata": {
        "id": "Pwk1Bu5hNtZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_in_parent_graph(state: State):\n",
        "  some_code() # <-- This will re-execute when the subgraph is resumed.\n",
        "  # Invoke a subgraph as a function.\n",
        "  # The subgraph contains an `interrupt` call.\n",
        "  subgraph_result = subgraph.invoke(some_result)\n",
        "  ..."
      ],
      "metadata": {
        "id": "juRhPHYj5pzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is abbreviated example code that you can use to understand how subgraphs work with interrupts. It counts the number of times each node is entered and prints the count."
      ],
      "metadata": {
        "id": "NefQjS6T99rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "from typing import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.constants import START\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.types import interrupt, Command\n",
        "\n",
        "class State(TypedDict):\n",
        "   \"\"\"The graph state.\"\"\"\n",
        "   some_text: str\n",
        "\n",
        "counter_node_in_subgraph = 0\n",
        "def node_in_subgraph(state: State):\n",
        "   global counter_node_in_subgraph\n",
        "   counter_node_in_subgraph += 1 # !!!This code will **NOT** run again!!!\n",
        "   print(f\"> (node_in_subgraph) Entered the node: {counter_node_in_subgraph} of times\")\n",
        "\n",
        "counter_huamn_node = 0\n",
        "def human_node(state: State):\n",
        "   global counter_huamn_node\n",
        "   counter_huamn_node += 1 #!!!This code will run again!!!\n",
        "   print(f\"> (huamn_node) Entered the node: {counter_huamn_node} of times\")\n",
        "   answer = interrupt(\"What is your name?\")\n",
        "   print(f\"Got an answer of {answer}\")\n",
        "\n",
        "subgraph_builder = StateGraph(State)\n",
        "subgraph_builder.add_node(\"node_in_subgraph\", node_in_subgraph)\n",
        "subgraph_builder.add_node(\"human_node\", human_node)\n",
        "subgraph_builder.add_edge(START, \"node_in_subgraph\")\n",
        "subgraph_builder.add_edge(\"node_in_subgraph\", \"human_node\")\n",
        "subgraph = subgraph_builder.compile(checkpointer=MemorySaver())\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "\n",
        "counter_parent_node = 0\n",
        "def parent_node(state: State):\n",
        "   \"\"\"This parent node will invoke the subgraph\"\"\"\n",
        "   global counter_parent_node\n",
        "   counter_parent_node += 1 #!!!This code will run again on resuming!!!\n",
        "   print(f\"> (parent_node) Entered the node: {counter_parent_node} of times\")\n",
        "   # Invoke the subgraph\n",
        "   subgraph_state = subgraph.invoke(state)\n",
        "   return subgraph_state\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_node(\"parent_node\", parent_node)\n",
        "graph_builder.add_edge(START, \"parent_node\")\n",
        "graph = graph_builder.compile(checkpointer=MemorySaver()) # A checkpointer must be enabled for interrupts to work\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(subgraph.get_graph().draw_mermaid_png()))\n",
        "print()\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "print()\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
        "for chunk in graph.stream({\"some_text\": 1}, config):\n",
        "  print(chunk)\n",
        "\n",
        "print()\n",
        "for chunk in graph.stream(Command(resume=\"18\"), config):\n",
        "  print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "24JIYxan96Uy",
        "outputId": "9dfab1f4-b00b-417d-cc69-d1580fb68dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> (parent_node) Entered the node: 1 of times\n",
            "> (node_in_subgraph) Entered the node: 1 of times\n",
            "> (huamn_node) Entered the node: 1 of times\n",
            "{'__interrupt__': (Interrupt(value='What is your name?', resumable=True, ns=['parent_node:e67ae280-4f8f-d72f-4640-73eea2c95533', 'human_node:31fa1255-dd5f-6265-0807-be5e01011328'], when='during'),)}\n",
            "\n",
            "> (parent_node) Entered the node: 2 of times\n",
            "> (huamn_node) Entered the node: 2 of times\n",
            "Got an answer of 18\n",
            "{'parent_node': {'some_text': 1}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.4.8.3.Using multiple interrupts"
      ],
      "metadata": {
        "id": "p9t36sgDN3Ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "from typing import TypedDict, Optional\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.constants import START\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.types import interrupt, Command\n",
        "\n",
        "class State(TypedDict):\n",
        "   \"\"\"The graph state.\"\"\"\n",
        "   age: Optional[str]\n",
        "   name: Optional[str]\n",
        "\n",
        "def human_node(state: State):\n",
        "  if not state.get('name'):\n",
        "    name = interrupt(\"What is your name?\")\n",
        "  else:\n",
        "    name = \"N/A\"\n",
        "\n",
        "  if not state.get('age'):\n",
        "    age = interrupt(\"What is your age?\")\n",
        "  else:\n",
        "    age = \"N/A\"\n",
        "\n",
        "  print(f\"Name: {name}, Age: {age}\")\n",
        "  return {\n",
        "    \"name\": name,\n",
        "    \"age\": age\n",
        "  }\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_node(\"human_node\", human_node)\n",
        "graph_builder.add_edge(START, \"human_node\")\n",
        "graph = graph_builder.compile(checkpointer=MemorySaver())\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
        "for chunk in graph.stream({\"age\": None, \"name\": None}, config):\n",
        "  print(chunk)\n",
        "\n",
        "print()\n",
        "for chunk in graph.stream(Command(resume=\"Hack\", update={\"name\": \"HackHuang\"}), config):\n",
        "  print(chunk)"
      ],
      "metadata": {
        "id": "qBGM0hpST-kI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d15b02-a4ba-4d06-e83a-5f2388c7adf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'__interrupt__': (Interrupt(value='What is your name?', resumable=True, ns=['human_node:51345894-6464-0ccd-3e87-ed2e24a374b5'], when='during'),)}\n",
            "\n",
            "Name: N/A, Age: Hack\n",
            "{'human_node': {'name': 'N/A', 'age': 'Hack'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.Persistence"
      ],
      "metadata": {
        "id": "kMr86Cl5Wknn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.Threads"
      ],
      "metadata": {
        "id": "lAcaDM6iWnnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.Storage"
      ],
      "metadata": {
        "id": "8tsDY7OXWpOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.Graph Migrations"
      ],
      "metadata": {
        "id": "yyx16YjWWsY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.Configuration"
      ],
      "metadata": {
        "id": "hNPpqRt9WxF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.interrupt"
      ],
      "metadata": {
        "id": "mpAhmHfEW08J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13.Breakpoints"
      ],
      "metadata": {
        "id": "fj9UP24tW2_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14.Subgraphs"
      ],
      "metadata": {
        "id": "zjvMycHSW5ty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15.Visualization"
      ],
      "metadata": {
        "id": "GPv9B9j3W8Ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16.Streaming"
      ],
      "metadata": {
        "id": "DqaHiViFW-ko"
      }
    }
  ]
}
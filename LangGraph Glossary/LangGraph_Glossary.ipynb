{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoogTech/langchain-tutorials/blob/master/LangGraph_Glossary/LangGraph_Glossary.ipynb\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Refer to the langgraph tutorials of `v0.2.74` : https://langchain-ai.github.io/langgraph/concepts/low_level/"
      ],
      "metadata": {
        "id": "KtmgCMiCVMCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "M1HIx4qkXkJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgRfVlAKXo5p",
        "outputId": "1414669f-d374-4193-8ccd-2bf5f8cb184b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.74-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.6-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.37)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.16-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.53-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.61.1)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.3.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.10.6)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Downloading langgraph-0.2.74-py3-none-any.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m151.4/151.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.6-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.16-py3-none-any.whl (38 kB)\n",
            "Downloading langgraph_sdk-0.1.53-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langgraph-sdk, langgraph-checkpoint, langchain-openai, langgraph\n",
            "Successfully installed langchain-openai-0.3.6 langgraph-0.2.74 langgraph-checkpoint-2.0.16 langgraph-sdk-0.1.53 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langgraph langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMPOP5t_X3GG",
        "outputId": "4b2fb12b-0290-439a-d9fc-cc3412fbc738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langgraph\n",
            "Version: 0.2.74\n",
            "Summary: Building stateful, multi-actor applications with LLMs\n",
            "Home-page: https://www.github.com/langchain-ai/langgraph\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, langgraph-checkpoint, langgraph-sdk\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-openai\n",
            "Version: 0.3.6\n",
            "Summary: An integration package connecting OpenAI and LangChain\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, openai, tiktoken\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Graphs"
      ],
      "metadata": {
        "id": "N3MZ17G8V0oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In short:\n",
        "* nodes do the work.\n",
        "* edges tell what to do next.\n",
        "* state represents the memory that records what you did."
      ],
      "metadata": {
        "id": "J4DtCJDwb1ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1.StateGraph"
      ],
      "metadata": {
        "id": "T0EHwIFxao3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.Compiling your graph"
      ],
      "metadata": {
        "id": "Ied3wE4qc57j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = graph_builder.compile(...)"
      ],
      "metadata": {
        "id": "O2145DM7eyXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.State"
      ],
      "metadata": {
        "id": "XWI3n13mV4Td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.Schema"
      ],
      "metadata": {
        "id": "rPpaU9Jhf8UQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1.How to use Pydantic model as graph state"
      ],
      "metadata": {
        "id": "MyufPjtsgYsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://langchain-ai.github.io/langgraph/how-tos/state-model/"
      ],
      "metadata": {
        "id": "c3Xwuo9Egp_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `StateGraph` accepts a state_schema argument on initialization `that` specifies the \"shape\" of the state `that` the nodes in the graph can access and update."
      ],
      "metadata": {
        "id": "Mq5mbP4ZmCXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input ValidationÔºö**"
      ],
      "metadata": {
        "id": "EAA-aeLthr0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# The overall state of the graph(this is the public state shared across nodes)\n",
        "class OverallState(BaseModel):\n",
        "  a: str\n",
        "\n",
        "def node(state: OverallState):\n",
        "  return {\"a\": \"goodbye\"}\n",
        "\n",
        "# Build the state graph:\n",
        "builder = StateGraph(OverallState)\n",
        "builder.add_node('node', node)\n",
        "builder.add_edge(START, 'node')\n",
        "builder.add_edge('node', END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# Test the graph with a valid input\n",
        "graph.invoke({\"a\": \"hello\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LJJ2Q77h2qr",
        "outputId": "4916513d-50ec-4d63-e36f-316655e37d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 'goodbye'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invoke the graph with an invalid input"
      ],
      "metadata": {
        "id": "LvZmUbgGj2eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  graph.invoke({\"a\": 123})\n",
        "except Exception as e:\n",
        "    print(\"An exception was raised because `a` is an integer rather than a string.\\n\")\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KMTqa2pj5Ff",
        "outputId": "e49dab76-2d92-4543-b63c-865f1a26c84d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An exception was raised because `a` is an integer rather than a string.\n",
            "\n",
            "1 validation error for OverallState\n",
            "a\n",
            "  Input should be a valid string [type=string_type, input_value=123, input_type=int]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple NodesÔºö**"
      ],
      "metadata": {
        "id": "dS99tINkkXNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# The overall state of the graph (this is the public state shared across nodes)\n",
        "class OverallState(BaseModel):\n",
        "    a: str\n",
        "\n",
        "def bad_node(state: OverallState):\n",
        "    return { # !!!returns an update to the state!!!\n",
        "        \"a\": 123  # Invalid\n",
        "    }\n",
        "\n",
        "def ok_node(state: OverallState):\n",
        "    return {\"a\": \"goodbye\"}\n",
        "\n",
        "# Build the state graph\n",
        "builder = StateGraph(OverallState)\n",
        "builder.add_node(bad_node)\n",
        "builder.add_node(ok_node)\n",
        "builder.add_edge(START, \"bad_node\")\n",
        "builder.add_edge(\"bad_node\", \"ok_node\")\n",
        "builder.add_edge(\"ok_node\", END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# Test the graph with a valid input\n",
        "try:\n",
        "    graph.invoke({\"a\": \"hello\"}) # !!!the validation error will occur when ok_node is called!!!\n",
        "except Exception as e:\n",
        "    print(\"An exception was raised because bad_node sets `a` to an integer.\")\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udrCOdVplMyG",
        "outputId": "bd40a6f5-8189-40da-9453-7cbbe32ca450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An exception was raised because bad_node sets `a` to an integer.\n",
            "1 validation error for OverallState\n",
            "a\n",
            "  Input should be a valid string [type=string_type, input_value=123, input_type=int]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2.How to define input/output schema for your graph"
      ],
      "metadata": {
        "id": "84zhFXB8gl0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://langchain-ai.github.io/langgraph/how-tos/input_output_schema/"
      ],
      "metadata": {
        "id": "nL9YZZMNgr5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema.\n",
        "\n",
        "However, it's also possible to define distinct input and output schemas for a graph."
      ],
      "metadata": {
        "id": "mDathDvVn9An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Define the schema for the input.\n",
        "# The input schema ensures that the provided input\n",
        "# matches the expected structure.\n",
        "class InputState(TypedDict):\n",
        "    question: str\n",
        "\n",
        "# Define the schema for the output.\n",
        "# The output schema filters the internal data to return\n",
        "# only the relevant information according to the defined output schema.\n",
        "class OutputState(TypedDict):\n",
        "    answer: str\n",
        "\n",
        "# Define the overall schema, combining both input and output\n",
        "class OverallState(InputState, OutputState):\n",
        "    pass\n",
        "\n",
        "# Define the node that processes the input and generates an answer\n",
        "def node(state: OverallState):\n",
        "    return {\"question\": state[\"question\"], \"answer\": \"This is answer...\"}\n",
        "\n",
        "# Build the graph with input and output schemas specified\n",
        "builder = StateGraph(OverallState, input=InputState, output=OutputState)\n",
        "builder.add_node(node)\n",
        "builder.add_edge(START, \"node\")\n",
        "builder.add_edge(\"node\", END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# Invoke the graph with and input and print the result.\n",
        "# Notice that the output of invoke only includes the output schema.\n",
        "print(graph.invoke({\"question\": \"What is the meaning of life?\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxzhCg9WpEI-",
        "outputId": "fff179a6-0c11-4126-899b-fdc168c6d185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'This is answer...'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple schemas"
      ],
      "metadata": {
        "id": "0a80W0Gfwg4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we define an \"internal\" schema that contains all keys relevant to graph operations.\n",
        "\n",
        "But, we also define input and output schemas that are sub-sets of the \"internal\" schema to constrain the input and output of the graph."
      ],
      "metadata": {
        "id": "x3SbELyxc28U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class InputState(TypedDict):\n",
        "  user_input: str # shared with the schema of `OverallState`\n",
        "\n",
        "class OutputState(TypedDict):\n",
        "  graph_output: str # shared with the schema of `OverallState`\n",
        "\n",
        "class OverallState(InputState, OutputState):\n",
        "  foo: str\n",
        "  user_input: str\n",
        "  graph_output: str\n",
        "\n",
        "class PrivateState(TypedDict):\n",
        "  bar: str # private variable, because any one of schema doesn't include it.\n",
        "\n",
        "#################################################################################################\n",
        "# Sate Process: User ‚Äî‚Äî‚Äî‚Äî> InputState ‚Äî‚Äî> OverallState ‚Äî‚Äî> PrivateSate ‚Äî‚Äî> OutputState ‚Äî‚Äî> User #\n",
        "#################################################################################################\n",
        "def node_1(state: InputState) -> OverallState:\n",
        "  # Read from InputState, write to OverallState\n",
        "  return {\"foo\": state[\"user_input\"] + \" name\"}\n",
        "\n",
        "def node_2(state: OverallState) -> PrivateState:\n",
        "  # Read from OverallState, write to PrivateState\n",
        "  return {\"bar\": state[\"foo\"] + \" is\"}\n",
        "\n",
        "def node_3(state: PrivateState) -> OutputState:\n",
        "  # Read from PrivateState, write to OutputState\n",
        "  return {\"graph_output\": state[\"bar\"] + \" Lance\"}\n",
        "\n",
        "builder = StateGraph(OverallState, input=InputState, output=OutputState)\n",
        "builder.add_node(node_1)\n",
        "builder.add_node(node_2)\n",
        "builder.add_node(node_3)\n",
        "builder.add_edge(START, \"node_1\")\n",
        "builder.add_edge(\"node_1\", \"node_2\")\n",
        "builder.add_edge(\"node_2\", \"node_3\")\n",
        "builder.add_edge(\"node_3\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "graph.invoke({\"user_input\": \"My\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV7HHqNBc6IT",
        "outputId": "c582fefa-4bdf-4064-8471-514e387fcce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'graph_output': 'My name is Lance'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to pass private state between nodes"
      ],
      "metadata": {
        "id": "hxBcSX8yxBOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# The overall state of the graph(this is the public state shared across nodes)\n",
        "class OverallState(TypedDict):\n",
        "  a: str\n",
        "\n",
        "# Output from node_1 contains private data that is not part of the overall state\n",
        "class Node1Output(TypedDict):\n",
        "  private_data: str\n",
        "\n",
        "# Node 2 input onlu requests the private data available after node_1\n",
        "class Node2Input(TypedDict):\n",
        "  private_data: str\n",
        "\n",
        "#################################################################################################################\n",
        "# Sate Process: User ‚Äî‚Äî> OverallState ‚Äî‚Äî> Node1Output ~~~ Node2Input ‚Äî‚Äî> OverallState ‚Äî‚Äî> OverallState ‚Äî‚Äî> User #\n",
        "#################################################################################################################\n",
        "# The private data is only shared between node_1 and node_2\n",
        "def node_1(state: OverallState) -> Node1Output:\n",
        "  output = {\"private_data\": \"set by node_1\"}\n",
        "  print(f\"Entered node `node_1`: \\n\\tInput: {state}.\\n\\tReturn:{output}\")\n",
        "  return output\n",
        "\n",
        "def node_2(state: Node2Input) -> OverallState:\n",
        "  output = {\"a\": \"set by node_2\"}\n",
        "  print(f\"Entered node `node_2`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n",
        "  return output\n",
        "\n",
        "# Node 3 only has access to the overall state (!!!no access to private data from node_1!!!)\n",
        "def node_3(state: OverallState) -> OverallState:\n",
        "  output = {\"a\": \"set by node_3\"}\n",
        "  print(f\"Entered node `node_3`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n",
        "  return output\n",
        "\n",
        "# Build the state graph\n",
        "builder = StateGraph(OverallState)\n",
        "builder.add_node(node_1) # node_1 is the first node\n",
        "builder.add_node(node_2) # node_2 is the second node and accepts private data from node_1\n",
        "builder.add_node(node_3) # node_3 is the third node and does not see the private data\n",
        "builder.add_edge(START, \"node_1\")\n",
        "builder.add_edge(\"node_1\", \"node_2\")\n",
        "builder.add_edge(\"node_2\", \"node_3\")\n",
        "builder.add_edge(\"node_3\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "# Invoke the graph with the initial state\n",
        "response = graph.invoke({\"a\": \"hello\"})\n",
        "print(f\"\\nOutput of graph invocation: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goeukIs405zW",
        "outputId": "d87e1e04-03fd-476e-a5da-11fa1892df5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entered node `node_1`: \n",
            "\tInput: {'a': 'hello'}.\n",
            "\tReturn:{'private_data': 'set by node_1'}\n",
            "Entered node `node_2`:\n",
            "\tInput: {'private_data': 'set by node_1'}.\n",
            "\tReturned: {'a': 'set by node_2'}\n",
            "Entered node `node_3`:\n",
            "\tInput: {'a': 'set by node_2'}.\n",
            "\tReturned: {'a': 'set by node_3'}\n",
            "\n",
            "Output of graph invocation: {'a': 'set by node_3'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.Reducers"
      ],
      "metadata": {
        "id": "ZBoopCPtnJnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example A:"
      ],
      "metadata": {
        "id": "5xVDP-E9pB6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "class State(TypedDict):\n",
        "    foo: int\n",
        "    bar: list[str]"
      ],
      "metadata": {
        "id": "C7nlq53HpEj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example B:"
      ],
      "metadata": {
        "id": "m84SfhFOp6uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import  TypedDict\n",
        "from operator import  add\n",
        "\n",
        "class State(TypedDict):\n",
        "    foo: int\n",
        "    bar: Annotated[list[str], add]"
      ],
      "metadata": {
        "id": "FAKaiLz5paAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.Working with Messages in Graph State"
      ],
      "metadata": {
        "id": "L-knyYDEqVF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.1.Messages"
      ],
      "metadata": {
        "id": "I0UdkO3d6LxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider."
      ],
      "metadata": {
        "id": "_YJVPk71yqWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The five main message types are :\n",
        "\n",
        "* SystemMessage: corresponds to `system` role\n",
        "* HumanMessage: corresponds to `user` role\n",
        "* AIMessage: corresponds to `assistant` role\n",
        "* AIMessageChunk: corresponds to `assistant` role, used for `streaming` responses\n",
        "* ToolMessage: corresponds to `tool` role"
      ],
      "metadata": {
        "id": "ZAUNVyHoyxSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HumanMessage :"
      ],
      "metadata": {
        "id": "g5Ro6C1Mzo7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HuamnMessage\n",
        "model.invoke([HuamanMessage(content=\"hello, how are you?\")])\n",
        "\n",
        "# Or:\n",
        "# When invoking a chat model with a string as input,\n",
        "# LangChain will automatically convert the string into a HumanMessage object.\n",
        "# This is mostly useful for quick testing.\n",
        "model.invoke(\"hello, how are you?\")"
      ],
      "metadata": {
        "id": "-EwR5ieNzUvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AIMessage :"
      ],
      "metadata": {
        "id": "qlVXxHv0zzti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "ai_message = model.invoke([HumanMessage(content=\"Tell me a joke\")])\n",
        "ai_message # <-- AIMessage"
      ],
      "metadata": {
        "id": "90tNwCeA0fAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AIMessageChunk :"
      ],
      "metadata": {
        "id": "MKxkYFcv1MUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in model.stream([HuamnMessage(\"What color is the sky?\")]):\n",
        "  print(chunk)"
      ],
      "metadata": {
        "id": "05jjSmzl1NdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI Format :\n",
        "Chat models also accept OpenAI's format as inputs to chat models"
      ],
      "metadata": {
        "id": "3WHj8-XM1yZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke([\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Hello, how are you?\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"I'm doing well, thank you for asking.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Can you tell me a joke?\",\n",
        "    }\n",
        "])"
      ],
      "metadata": {
        "id": "BgEi9ODV11C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.2.Serialization"
      ],
      "metadata": {
        "id": "w91I9DrVtIH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AnyMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class State(TypedDict):\n",
        "  # the state updates are always deserialized into LangChain Messages\n",
        "  # when using add_messages\n",
        "  messages: Annotated[list[AnyMessage], add_messages]"
      ],
      "metadata": {
        "id": "0YZ_gfc75KbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.3.MessagesState"
      ],
      "metadata": {
        "id": "-AcJ-hcP6YTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "\n",
        "# MessagesState is defined with a single messages key\n",
        "# which is a list of AnyMessage objects and uses the add_messages reducer.\n",
        "class State(MessagesState):\n",
        "  # Typically, there is more state to track than just messages,\n",
        "  # so we see people subclass this state and add more fields, like:\n",
        "  documents: list[str]"
      ],
      "metadata": {
        "id": "UwPFx1_v6ttM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Nodes"
      ],
      "metadata": {
        "id": "JIIoUWVbV-sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "builder = StateGraph(dict)https://langchain-ai.github.io/langgraph/concepts/low_level/#start-node\n",
        "\n",
        "# the first positional argument is the state,\n",
        "# and (optionally), the second positional argument is a \"config\",\n",
        "def my_node(state: dict, config: RunnableConfig):\n",
        "  print(\"In node:\", config[\"configurable\"][\"user_id\"])\n",
        "  return {\"results\": f\"Hello, {state['input']}\"}\n",
        "\n",
        "# The second argument is optional\n",
        "def my_other_node(state: dict):\n",
        "  return state\n",
        "\n",
        "builder.add_node('my_node', my_node)\n",
        "# If you add a node to a graph without specifying a name,\n",
        "# it will be given a default name equivalent to the function name.\n",
        "builder.add_node(my_other_node)"
      ],
      "metadata": {
        "id": "d9yjdvy_9Kf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.START Node"
      ],
      "metadata": {
        "id": "ee80h594-uqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START\n",
        "\n",
        "#  The main purpose for referencing this node is\n",
        "# to determine which nodes should be called first.\n",
        "builder.add_edge(START, \"my_node\")"
      ],
      "metadata": {
        "id": "8goIq7AY-3xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.END Node"
      ],
      "metadata": {
        "id": "2NiS_zxQ_LED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END\n",
        "\n",
        "# This node is referenced when you want to denote\n",
        "# which edges have no actions after they are done.\n",
        "graph.add_edge(\"node_a\", END)"
      ],
      "metadata": {
        "id": "1a7HnIxG_OXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Edges"
      ],
      "metadata": {
        "id": "aPJ8WEpeWa1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.Normal Edges\n"
      ],
      "metadata": {
        "id": "oeuJCVZFAr2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.add_edge(\"node_a\", \"node_b\")"
      ],
      "metadata": {
        "id": "m3FE7yGiAv-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.Conditional Edges"
      ],
      "metadata": {
        "id": "z6_ty0naA1Qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# By default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next.\n",
        "# All those nodes will be run in parallel as a part of the next superstep.\n",
        "graph.add_conditional_edges(\"node_a\", routing_function)\n",
        "\n",
        "# Or you can optionally provide a dictionary that maps the routing_function's output to the name of the next node.\n",
        "graph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_a\"})"
      ],
      "metadata": {
        "id": "6goaMoUlA6YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Send"
      ],
      "metadata": {
        "id": "taltLzySWeBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def continue_to_jokes(state: OverallState):\n",
        "  # Send takes two arguments:\n",
        "  # first is the name of the node, and second is the state to pass to that node.\n",
        "  return [Send(\"generate_joke\", {\"subject\": s}) for s in state['subjects']]\n",
        "\n",
        "graph.add_conditional_edges('node_a', continue_to_jokes)"
      ],
      "metadata": {
        "id": "xxAESxXdFTQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Command"
      ],
      "metadata": {
        "id": "D8xkHEP1WgJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  you might want to BOTH perform state updates AND decide\n",
        "# which node to go to next in the SAME node.\n",
        "def my_node(state: State) -> Command[Literals[\"my_other_node\"]]:\n",
        "  return Command(\n",
        "      # state udpate\n",
        "      update = (\"foo\": \"bar\"),\n",
        "      # control flow\n",
        "      goto=\"my_other_node\"\n",
        "  )\n",
        "\n",
        "# With `Command` you can also achieve dynamic control flow behavior (identical to conditional edges)\n",
        "def my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n",
        "  if state[\"foo\"] == \"bar\":\n",
        "    return Command(\n",
        "        update = {\"foo\": \"baz\"},\n",
        "        goto = \"my_other_node\"\n",
        "    )"
      ],
      "metadata": {
        "id": "O1OR0_XGHcer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.When should I use Command instead of conditional edges?"
      ],
      "metadata": {
        "id": "_Oq_0w3-KC94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Command when you need to both update the graph state and route to a different node.\n",
        "\n",
        "For example, when implementing multi-agent handoffs where it's important to route to a different agent and pass some information to that agent."
      ],
      "metadata": {
        "id": "_QrxKNt_KYsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2.Navigating to a node in a parent graph"
      ],
      "metadata": {
        "id": "W_JwIMmNL03a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_node(state: State) -> Command[Literal['my_other_node']]:\n",
        "  return Command(\n",
        "      update = {\"foo\": \"bar\"},\n",
        "      goto = \"my_other_node\", # # where `other_subgraph` is a node in the parent graph\n",
        "      graph = Command.PARENT\n",
        "  )"
      ],
      "metadata": {
        "id": "ITxc_1rRNvzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.Using inside tools"
      ],
      "metadata": {
        "id": "3l8CtX1yQKKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\n",
        "    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\n",
        "    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\n",
        "    return Command(\n",
        "        update={\n",
        "            # update the state keys\n",
        "            \"user_info\": user_info,\n",
        "            # update the message history\n",
        "            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "KId4WF6iQSSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.Persistence"
      ],
      "metadata": {
        "id": "kMr86Cl5Wknn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.Threads"
      ],
      "metadata": {
        "id": "lAcaDM6iWnnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.Storage"
      ],
      "metadata": {
        "id": "8tsDY7OXWpOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.Graph Migrations"
      ],
      "metadata": {
        "id": "yyx16YjWWsY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.Configuration"
      ],
      "metadata": {
        "id": "hNPpqRt9WxF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.interrupt"
      ],
      "metadata": {
        "id": "mpAhmHfEW08J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13.Breakpoints"
      ],
      "metadata": {
        "id": "fj9UP24tW2_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14.Subgraphs"
      ],
      "metadata": {
        "id": "zjvMycHSW5ty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15.Visualization"
      ],
      "metadata": {
        "id": "GPv9B9j3W8Ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16.Streaming"
      ],
      "metadata": {
        "id": "DqaHiViFW-ko"
      }
    }
  ]
}
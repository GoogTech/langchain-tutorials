{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoogTech/langchain-tutorials/blob/master/Chatbots/Build_a_Chatbot.ipynb\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "2Mb8zdY2i3Vn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVCy6s5FG_vd",
        "outputId": "40d7ed98-ee7f-49cb-a7ea-94d1ba08cc82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.17)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.34)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.2.70)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.12)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.12)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.51)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.61.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.8.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-core langgraph langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hywCfz4IWL4",
        "outputId": "823c5bc2-5d1e-427c-d16c-70bb958be20e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.3.17\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: aiohttp, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-openai\n",
            "Version: 0.3.4\n",
            "Summary: An integration package connecting OpenAI and LangChain\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, openai, tiktoken\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quickstart"
      ],
      "metadata": {
        "id": "Wo6JjwdRLX1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import HumanMessage\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "model = init_chat_model(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    model_provider=\"openai\",\n",
        "    openai_api_key=userdata.get('OPENAI_API_KEY'),\n",
        "    base_url=userdata.get('BASE_URL'),\n",
        ")\n",
        "\n",
        "model.invoke([HumanMessage(content=\"Hi! I'm Hack\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xuj9I9bHHsFT",
        "outputId": "c64624d8-895c-493d-d636-c6a14f02c58c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi, Hack! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 11, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f3927aa00d', 'finish_reason': 'stop', 'logprobs': None}, id='run-47a18687-01a7-42db-992b-2d7942b04c80-0', usage_metadata={'input_tokens': 11, 'output_tokens': 11, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke([HumanMessage(content=\"What's my name?\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtN827fvK2oh",
        "outputId": "6ba87ac9-9fbc-4354-f7f1-bf819cc4d92c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but I don't have access to personal data about users unless it's shared with me in the course of our conversation. If you'd like me to know your name, feel free to tell me!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 11, 'total_tokens': 51, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f3927aa00d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f1b129be-bc2f-478e-89cb-1d2cdcc716eb-0', usage_metadata={'input_tokens': 11, 'output_tokens': 40, 'total_tokens': 51, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
        "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
        "        HumanMessage(content=\"What's my name?\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTndwTgCLGEO",
        "outputId": "2c7b91d6-06b0-4b69-a64c-f70b33c35395"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Bob. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 33, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f3927aa00d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f8abb00a-a5f0-4798-806a-4c0a1eb0e853-0', usage_metadata={'input_tokens': 33, 'output_tokens': 12, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Message persistence"
      ],
      "metadata": {
        "id": "2QR24B4kLahF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "  response = model.invoke(state[\"messages\"])\n",
        "  return {\"messages\": response}\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "G-RdgE66Lk-8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"thread_id_123\"}}\n",
        "\n",
        "query = \"Hi! I'm Hack\"\n",
        "\n",
        "input_messages = HumanMessage(content=query)\n",
        "response = app.invoke({\"messages\": input_messages}, config)\n",
        "response[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDK6J2bQMpfe",
        "outputId": "4c2ac13b-3288-4cb2-af60-0d4832661646"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi, Hack! It's great to meet you. How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"thread_id_123\"}}\n",
        "\n",
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = HumanMessage(content=query)\n",
        "response = app.invoke({\"messages\": input_messages}, config)\n",
        "response[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_kBRkzKNoJq",
        "outputId": "b3967094-fb8f-4c61-ad6c-8814b17cd975"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Hack! How can I help you today, Hack?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"thread_id_789\"}} # different thread_id\n",
        "\n",
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = HumanMessage(content=query)\n",
        "response = app.invoke({\"messages\": input_messages}, config)\n",
        "response[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhBsqzjrOZ1I",
        "outputId": "e4b3ce70-5cab-488e-e3b2-a058d0708f00"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I'm sorry, but I don't have any information about your name. If you'd like to share it, I can certainly address you by it!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates"
      ],
      "metadata": {
        "id": "mIGHBFXkPLZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Define the prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "  # response = model.invoke(state[\"messages\"])\n",
        "  prompt = prompt_template.invoke(state)\n",
        "  response = model.invoke(prompt)\n",
        "  return {\"messages\": response}\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "ouFghRDNXhDs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"thread_id_666\"}}\n",
        "\n",
        "query = \"Hi! I'm Hack\"\n",
        "\n",
        "input_messages = HumanMessage(content=query)\n",
        "response = app.invoke({\"messages\": input_messages}, config)\n",
        "response[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFO5Gz6sYuCO",
        "outputId": "03c66acf-a65e-4b38-9a25-e75400c43c1f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Ahoy there, Hack! What be bringin' ye to these digital seas today? Be ye lookin' for treasure, knowledge, or a hearty yarn?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"thread_id_666\"}}\n",
        "\n",
        "query = \"hat is my name?\"\n",
        "\n",
        "input_messages = HumanMessage(content=query)\n",
        "response = app.invoke({\"messages\": input_messages}, config)\n",
        "response[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf8cjr_DZi3x",
        "outputId": "4e46c145-5689-4f5c-dafa-6bdc060ba666"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Yer name be Hack, savvy? A fine name fer a landlubber or a scallywag alike! What else can I help ye with, matey?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "from typing import Sequence\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "# Define the prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    language: str\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=State)\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: State):\n",
        "  # response = model.invoke(state[\"messages\"])\n",
        "  prompt = prompt_template.invoke(state)\n",
        "  response = model.invoke(prompt)\n",
        "  return {\"messages\": response}\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "DerL_GltZ0qt"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc888\"}}\n",
        "\n",
        "query = \"Hi! I'm Hack\"\n",
        "language = \"Chinese\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "response = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")\n",
        "response[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlg0KG0TacRW",
        "outputId": "b4c82636-a3cf-485f-a282-3c4654444bc9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "你好，Hack！有什么我可以帮助你的吗？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc888\"}}\n",
        "\n",
        "query = \"What is my name?\"\n",
        "language = \"Chinese\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "response = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")\n",
        "response[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_brhRfI4axNh",
        "outputId": "b802b39d-af59-42b1-aefd-27aa7c722d0c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "你的名字是Hack。有什么其他问题吗？\n"
          ]
        }
      ]
    }
  ]
}